# ğŸ§  Machine Learning Experiment â€” Support Vector Machine (SVM) Classification (Lab 4)

ğŸ“˜ **Project Overview**  
This folder contains the full implementation, visualization results, and report for the **fourth machine learning experiment**, which focuses on **Support Vector Machine (SVM)** classification using linear kernels.  
The study explores how **data separability**, **margin width**, and **regularization parameter C** influence the decision boundary, support vectors, and overall classification performance.

---

## âš™ï¸ Folder Structure

```plaintext
.
â”œâ”€â”€ lab1.py                             # Requirement 1: Basic linear SVM training (C = 1e5)
â”œâ”€â”€ lab2.py                             # Extended task: parameter tuning for C (0.01 â€“ 10000)
â”œâ”€â”€ log.txt                             # Support-vector b-values for each C in the extended task
â”œâ”€â”€ è¦æ±‚1-1.png                          # Center (6, 6): training data distribution
â”œâ”€â”€ è¦æ±‚1-2.png                          # Center (6, 6): prediction distribution
â”œâ”€â”€ è¦æ±‚1-3.png                          # Center (6, 6): support-vector b-values
â”œâ”€â”€ è¦æ±‚1-4.png                          # Center (3, 3): training data distribution
â”œâ”€â”€ è¦æ±‚1-5.png                          # Center (3, 3): prediction distribution
â”œâ”€â”€ è¦æ±‚1-6.png                          # Center (3, 3): support-vector b-values
â”œâ”€â”€ é™„åŠ é¢˜3-1.png                        # Center (6, 6): predictions under C = 0.01, 0.1, 1, 10000
â”œâ”€â”€ é™„åŠ é¢˜3-2.png                        # Center (3, 3): predictions under C = 0.01, 0.1, 1, 10000
```

---

## ğŸš€ Features

- âš™ï¸ **Linear SVM Implementation**
  - Uses `sklearn.svm.SVC(kernel='linear')` for clear visualization of hyperplanes.  
  - Displays classification surfaces, margin boundaries, and all support vectors.

- ğŸ§© **Two Experimental Scenarios**
  1. **Requirement 1:**  
     - Second-class center = (6, 6) and (3, 3).  
     - Visualizes data distribution, classification boundary, and computed b-values.
  2. **Extended Task 3:**  
     - Investigates the effect of different C values (0.01 / 0.1 / 1 / 10000).  
     - Shows how margin width and misclassification tolerance change.  
     - Logs all support-vector b-values to `log.txt`.

- ğŸ“ˆ **Analysis Highlights**
  - Smaller C â†’ larger margin, higher tolerance â†’ potential underfitting.  
  - Larger C â†’ narrower margin, stricter constraint â†’ risk of overfitting.  
  - Support vectors remain critical for defining the decision boundary.

---

## ğŸ§© Code Overview

### ğŸ”¹ Requirement 1 (lab1.py)

```python
from sklearn.svm import SVC
import numpy as np, matplotlib.pyplot as plt

n = 100
center1, center2 = np.array([1,1]), np.array([6,6])
X = np.vstack([center1 + np.random.randn(n,2),
               center2 + np.random.randn(n,2)])
Y = np.hstack([np.ones(n), -np.ones(n)])

clf = SVC(kernel='linear', C=1e5)
clf.fit(X, Y)

w, b = clf.coef_[0], clf.intercept_[0]
support_vectors = clf.support_vectors_
calculated_bs = Y[clf.support_] - np.dot(support_vectors, w)
print("æ”¯æŒå‘é‡è®¡ç®—å‡ºçš„ b å€¼:", calculated_bs)
```

### ğŸ”¹ Extended Task 3 (lab2.py)

```python
def plot_svm_with_different_C(C_values, X, Y):
    for i, C in enumerate(C_values):
        clf = SVC(kernel='linear', C=C)
        clf.fit(X, Y)
        w, b = clf.coef_[0], clf.intercept_[0]
        # Plot decision boundary & margins â€¦
C_values = [0.01, 0.1, 1, 10000]
plot_svm_with_different_C(C_values, X, Y)
```

---

## ğŸ“Š Experimental Findings

| Scenario | Data Center | C Value Range | Key Observation | Margin Behavior |
|-----------|--------------|---------------|-----------------|----------------|
| Requirement 1 | (6, 6) / (3, 3) | 1e5 | Stable separation; support vectors clearly defined | Wide margin |
| Extended Task 3 | (6, 6) / (3, 3) | 0.01 â€“ 10000 | C â†‘ â†’ margin â†“, error â†“, risk of overfitting | Adjustable |

**Summary of B-values (log.txt):**  
- Smaller C â†’ B values more scattered (soft margin tolerance).  
- Larger C â†’ B values converge (closer to hard margin solution).  

---

## ğŸ§  Tech Stack

- **Language:** Python 3.10+  
- **Libraries:** NumPy, Matplotlib, scikit-learn  
- **Algorithm:** Support Vector Machine (SVM, Linear Kernel)  
- **Environment:** Jupyter Notebook / Python CLI  

---

## ğŸš€ Getting Started

1. **Clone Repository**
   ```bash
   git clone https://github.com/ali-rose/Bachelor_project.git
   cd Bachelor_project/Machine_learning/lab4
   ```

2. **Install Dependencies**
   ```bash
   pip install numpy matplotlib scikit-learn
   ```

3. **Run Experiments**
   ```bash
   python lab1.py   # Requirement 1 â€“ basic SVM
   python lab2.py   # Extended Task 3 â€“ C-parameter tuning
   ```

4. **View Results**
   - Images `è¦æ±‚1-*.png` â†’ center (6, 6) and (3, 3) comparisons  
   - Images `é™„åŠ é¢˜3-*.png` â†’ C-parameter visualization  
   - `log.txt` â†’ support-vector b-values  

---

## ğŸ”— Original Reference

Based on the foundational work of:  
ğŸ‘‰ *Cortes, C. & Vapnik, V. (1995). â€œSupport Vector Networks.â€ Machine Learning 20(3): 273-297.*

---

## ğŸ“„ References

- scikit-learn Documentation â€” [https://scikit-learn.org](https://scikit-learn.org)  
- NumPy Documentation â€” [https://numpy.org](https://numpy.org)  
- Matplotlib Documentation â€” [https://matplotlib.org](https://matplotlib.org)  

---

## ğŸ§© Disclaimer

This project is for **educational and research purposes only**.  
All rights belong to their respective authors.

---

## ğŸ‘¨â€ğŸ’» Author

**Ailixiaer Ailika**  
Bachelor Thesis Project â€” *Machine Learning Fundamentals (Lab 4)*  
ğŸ“ University Project Repository (Non-Commercial Use)

---

## ğŸªª License

Released under the **MIT License**.  
Free to use, modify, and redistribute with attribution.
