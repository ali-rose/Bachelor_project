# ğŸ§  Machine Learning Experiment â€” K-Nearest Neighbors (KNN) Classification (Lab 2)

ğŸ“˜ **Project Overview**  
This folder contains the complete implementation, visualization results, and written report for the **second machine learning experiment**, which studies the **K-Nearest Neighbors (KNN)** algorithm.  
The experiment investigates how **distance metrics**, **training sample size**, and **the number of neighbors K** influence classification accuracy and generalization on synthetic two-dimensional data.

---

## âš™ï¸ Folder Structure

```plaintext
.
â”œâ”€â”€ lab1.py                               # Code for Requirement 1, Extra 1, Extra 2
â”œâ”€â”€ lab2.py                               # Code for Extra 3 (K-value analysis)
â”œâ”€â”€ è¦æ±‚1-1.png                            # Requirement 1 â€“ Training data distribution
â”œâ”€â”€ è¦æ±‚1-2.png                            # Requirement 1 â€“ Test data distribution
â”œâ”€â”€ è¦æ±‚1-3.png                            # Requirement 1 â€“ Predicted data distribution
â”œâ”€â”€ è¦æ±‚1-4.png                            # Requirement 1 â€“ Error-rate visualization
â”œâ”€â”€ é™„åŠ é¢˜1-1.png                          # Extra 1 â€“ Training data distribution
â”œâ”€â”€ é™„åŠ é¢˜1-2.png                          # Extra 1 â€“ Test data distribution
â”œâ”€â”€ é™„åŠ é¢˜1-3.png                          # Extra 1 â€“ Predicted data distribution
â”œâ”€â”€ é™„åŠ é¢˜1-4.png                          # Extra 1 â€“ Error-rate visualization
â”œâ”€â”€ é™„åŠ é¢˜2-1.png â€“ é™„åŠ é¢˜2-4.png            # Extra 2 â€“ Manhattan distance: train/test/predict/error
â”œâ”€â”€ é™„åŠ é¢˜2-5.png â€“ é™„åŠ é¢˜2-8.png            # Extra 2 â€“ Chebyshev distance: train/test/predict/error
â”œâ”€â”€ é™„åŠ é¢˜3-1.png                          # Extra 3 â€“ Training data distribution
â”œâ”€â”€ é™„åŠ é¢˜3-2.png                          # Extra 3 â€“ Error rate vs K curve
```

---

## ğŸš€ Features

- ğŸ§© **KNN Implementation (from scratch)**  
  - Manual computation of Euclidean, Manhattan, and Chebyshev distances.  
  - Majority-vote prediction without external ML libraries.

- ğŸ” **Experiments Conducted**
  1. **Requirement 1:**  
     Baseline KNN classifier using Euclidean distance.  
     Images 1â€“4 â†’ training / testing / prediction / error rate.
  2. **Extra 1:**  
     Reduced training sample size to analyze generalization behavior (100 train vs 500 test).  
     Images 1â€“4 â†’ training / testing / prediction / error rate.
  3. **Extra 2:**  
     Comparison of **Manhattan** vs **Chebyshev** distances.  
     - 1â€“4 â†’ Manhattan results  
     - 5â€“8 â†’ Chebyshev results.
  4. **Extra 3:**  
     Influence of **K value** (1â€“100) on accuracy.  
     Images 1â€“2 â†’ training data and K-error curve.

- ğŸ“ˆ **Evaluation Metrics**  
  - Classification accuracy and error rate.  
  - Visual comparison of decision boundaries.

---

## ğŸ§© Code Overview

### ğŸ”¹ Core KNN Function
```python
def k_nearest_neighbors(X_train, y_train, x_test, k, dist_fn):
    distances = [dist_fn(x_train, x_test) for x_train in X_train]
    k_idx = np.argsort(distances)[:k]
    labels = y_train[k_idx]
    values, counts = np.unique(labels, return_counts=True)
    return values[np.argmax(counts)]
```

### ğŸ”¹ Distance Metrics
```python
def euclidean(a, b):  return np.sqrt(np.sum((a - b)**2))
def manhattan(a, b):  return np.sum(np.abs(a - b))
def chebyshev(a, b):  return np.max(np.abs(a - b))
```

### ğŸ”¹ Error-Rate Calculation
```python
pred = [k_nearest_neighbors(X_train, y_train, x, k, euclidean) for x in X_test]
error = np.mean(pred != y_test)
print(f"Error rate: {error:.3f}")
```

---

## ğŸ“Š Experimental Findings

| Experiment | Distance | Training / Test | Best K | Error Rate |
|-------------|-----------|-----------------|--------|-------------|
| Requirement 1 | Euclidean | 500 / 100 | 3 | 0.00 |
| Extra 1 | Euclidean | 100 / 500 | 3 | 0.05 |
| Extra 2-1 | Manhattan | 100 / 500 | 5 | 0.03 |
| Extra 2-2 | Chebyshev | 100 / 500 | 5 | 0.11 |
| Extra 3 | Euclidean | 500 / 100 | 1â€“100 | Best â‰ˆ K = 3â€“5 |

**Key Insights**
- Increasing K smooths boundaries but may underfit.  
- Manhattan distance performs better on sparse grids.  
- Too few training samples increase variance and error.  
- Chebyshev distance exaggerates axis-aligned bias.

---

## ğŸ§  Tech Stack

- **Language:** Python 3.10+  
- **Libraries:** NumPy, Matplotlib  
- **Dataset:** Synthetic 2-D data (0â€“10 grid)  
- **Algorithm:** K-Nearest Neighbors (Classic Model)  
- **Environment:** Jupyter Notebook / Command Line  

---

## ğŸš€ Getting Started

1. **Clone Repository**
   ```bash
   git clone https://github.com/ali-rose/Bachelor_project.git
   cd Bachelor_project/Machine_learning/lab2
   ```
2. **Install Dependencies**
   ```bash
   pip install numpy matplotlib
   ```
3. **Run Experiments**
   ```bash
   python lab1.py   # Requirement 1 â€“ Extra 2
   python lab2.py   # Extra 3 (K-value analysis)
   ```
4. **View Results**
   - Visuals â†’ `è¦æ±‚1-*.png`, `é™„åŠ é¢˜*-*.png`  

---

## ğŸ”— Original Reference

Based on the classic KNN framework from  
ğŸ‘‰ *T. M. Cover & P. E. Hart (1967), â€œNearest Neighbor Pattern Classification,â€ IEEE Trans. Info. Theory.*

---

## ğŸ“„ References

- NumPy Docs â€” [https://numpy.org](https://numpy.org)  
- Matplotlib Docs â€” [https://matplotlib.org](https://matplotlib.org)

---

## ğŸ§© Disclaimer

This project is for **educational and research use only**.  
All rights belong to their original authors.

---

## ğŸ‘¨â€ğŸ’» Author

**Ailixiaer Ailika**  
Bachelor Thesis Project â€” *Machine Learning Fundamentals (Lab 2)*  
ğŸ“ University Project Repository (Non-Commercial Use)

---

## ğŸªª License

Released under the **MIT License**.  
Free to use, modify, and redistribute with attribution.
